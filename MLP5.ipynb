{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81e5fcd-992c-4e18-9619-ad2b95f36712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                1600      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64)                256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4694 (18.34 KB)\n",
      "Trainable params: 4502 (17.59 KB)\n",
      "Non-trainable params: 192 (768.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Assume each base model gives a (1,6) output (6-class softmax probabilities)\n",
    "num_models = 4\n",
    "num_classes = 6  # Softmax output size from each base model\n",
    "\n",
    "# Correct input shape: 4 models * 6 outputs = 24 features\n",
    "input_shape = num_models * num_classes  # 24\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "meta_classifier = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(input_shape,)),  \n",
    "    BatchNormalization(),  # Normalize inputs for faster learning\n",
    "    Dropout(0.3),  # Reduce overfitting\n",
    "\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "\n",
    "    Dense(num_classes, activation='softmax')  # Output Layer (6 pollution categories)\n",
    "])\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# AdamW is better than Adam due to decoupled weight decay\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=1e-5)\n",
    "\n",
    "meta_classifier.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "meta_classifier.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9a56b4-29ed-48e6-b8d6-685e2a4b548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 1.8646 - accuracy: 0.1562 - val_loss: 1.7939 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 2/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8521 - accuracy: 0.1675 - val_loss: 1.7929 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 3/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8608 - accuracy: 0.1675 - val_loss: 1.7917 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 4/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8603 - accuracy: 0.1625 - val_loss: 1.7914 - val_accuracy: 0.1750 - lr: 6.2500e-05\n",
      "Epoch 5/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8587 - accuracy: 0.1637 - val_loss: 1.7907 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 6/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8336 - accuracy: 0.1787 - val_loss: 1.7905 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 7/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8555 - accuracy: 0.1700 - val_loss: 1.7893 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 8/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8513 - accuracy: 0.1787 - val_loss: 1.7881 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 9/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8589 - accuracy: 0.1725 - val_loss: 1.7872 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 10/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8683 - accuracy: 0.1363 - val_loss: 1.7871 - val_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 11/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8409 - accuracy: 0.1950 - val_loss: 1.7867 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 12/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8446 - accuracy: 0.1700 - val_loss: 1.7860 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 13/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8301 - accuracy: 0.1762 - val_loss: 1.7854 - val_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 14/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8340 - accuracy: 0.1787 - val_loss: 1.7848 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 15/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8414 - accuracy: 0.1800 - val_loss: 1.7845 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 16/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.8283 - accuracy: 0.1762 - val_loss: 1.7849 - val_accuracy: 0.1450 - lr: 6.2500e-05\n",
      "Epoch 17/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8462 - accuracy: 0.1775 - val_loss: 1.7851 - val_accuracy: 0.1450 - lr: 6.2500e-05\n",
      "Epoch 18/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8380 - accuracy: 0.1737 - val_loss: 1.7850 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 19/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8254 - accuracy: 0.2062 - val_loss: 1.7854 - val_accuracy: 0.1550 - lr: 6.2500e-05\n",
      "Epoch 20/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8326 - accuracy: 0.1637 - val_loss: 1.7845 - val_accuracy: 0.1500 - lr: 6.2500e-05\n",
      "Epoch 21/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8165 - accuracy: 0.2000 - val_loss: 1.7833 - val_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 22/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8208 - accuracy: 0.1737 - val_loss: 1.7830 - val_accuracy: 0.1600 - lr: 6.2500e-05\n",
      "Epoch 23/300\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 1.8256 - accuracy: 0.1737 - val_loss: 1.7829 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 24/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8162 - accuracy: 0.2000 - val_loss: 1.7826 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 25/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8322 - accuracy: 0.1737 - val_loss: 1.7819 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 26/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8204 - accuracy: 0.1863 - val_loss: 1.7819 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 27/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8259 - accuracy: 0.1625 - val_loss: 1.7816 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 28/300\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.8292 - accuracy: 0.1838 - val_loss: 1.7814 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 29/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8204 - accuracy: 0.1900 - val_loss: 1.7812 - val_accuracy: 0.1650 - lr: 6.2500e-05\n",
      "Epoch 30/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8148 - accuracy: 0.1937 - val_loss: 1.7809 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 31/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8198 - accuracy: 0.1562 - val_loss: 1.7811 - val_accuracy: 0.1750 - lr: 6.2500e-05\n",
      "Epoch 32/300\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.8097 - accuracy: 0.1850 - val_loss: 1.7806 - val_accuracy: 0.1800 - lr: 6.2500e-05\n",
      "Epoch 33/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.8106 - accuracy: 0.1912 - val_loss: 1.7802 - val_accuracy: 0.1800 - lr: 6.2500e-05\n",
      "Epoch 34/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8252 - accuracy: 0.1975 - val_loss: 1.7810 - val_accuracy: 0.1750 - lr: 6.2500e-05\n",
      "Epoch 35/300\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.8101 - accuracy: 0.1787 - val_loss: 1.7803 - val_accuracy: 0.1800 - lr: 6.2500e-05\n",
      "Epoch 36/300\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.8213 - accuracy: 0.1787 - val_loss: 1.7802 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 37/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8160 - accuracy: 0.1725 - val_loss: 1.7795 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 38/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8119 - accuracy: 0.1887 - val_loss: 1.7792 - val_accuracy: 0.1900 - lr: 6.2500e-05\n",
      "Epoch 39/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8061 - accuracy: 0.2013 - val_loss: 1.7790 - val_accuracy: 0.1900 - lr: 6.2500e-05\n",
      "Epoch 40/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8107 - accuracy: 0.1950 - val_loss: 1.7789 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 41/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8160 - accuracy: 0.1887 - val_loss: 1.7795 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 42/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8045 - accuracy: 0.1787 - val_loss: 1.7801 - val_accuracy: 0.1900 - lr: 6.2500e-05\n",
      "Epoch 43/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8034 - accuracy: 0.1950 - val_loss: 1.7801 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 44/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7973 - accuracy: 0.1937 - val_loss: 1.7802 - val_accuracy: 0.1850 - lr: 6.2500e-05\n",
      "Epoch 45/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.8118 - accuracy: 0.1850 - val_loss: 1.7812 - val_accuracy: 0.1900 - lr: 6.2500e-05\n",
      "Epoch 46/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7913 - accuracy: 0.2138 - val_loss: 1.7810 - val_accuracy: 0.1800 - lr: 6.2500e-05\n",
      "Epoch 47/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8176 - accuracy: 0.1850 - val_loss: 1.7811 - val_accuracy: 0.1800 - lr: 6.2500e-05\n",
      "Epoch 48/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7900 - accuracy: 0.1950 - val_loss: 1.7808 - val_accuracy: 0.1700 - lr: 6.2500e-05\n",
      "Epoch 49/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.8002 - accuracy: 0.1912 - val_loss: 1.7812 - val_accuracy: 0.1750 - lr: 6.2500e-05\n",
      "Epoch 50/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7972 - accuracy: 0.1988 - val_loss: 1.7815 - val_accuracy: 0.1750 - lr: 3.1250e-05\n",
      "Epoch 51/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7954 - accuracy: 0.1925 - val_loss: 1.7816 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 52/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.7886 - accuracy: 0.1950 - val_loss: 1.7821 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 53/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8018 - accuracy: 0.2013 - val_loss: 1.7822 - val_accuracy: 0.1850 - lr: 3.1250e-05\n",
      "Epoch 54/300\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 1.7983 - accuracy: 0.1975 - val_loss: 1.7824 - val_accuracy: 0.1900 - lr: 3.1250e-05\n",
      "Epoch 55/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7944 - accuracy: 0.1950 - val_loss: 1.7824 - val_accuracy: 0.1850 - lr: 3.1250e-05\n",
      "Epoch 56/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7877 - accuracy: 0.1975 - val_loss: 1.7826 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 57/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7801 - accuracy: 0.2212 - val_loss: 1.7828 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 58/300\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 1.7945 - accuracy: 0.2188 - val_loss: 1.7828 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 59/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.7879 - accuracy: 0.1925 - val_loss: 1.7824 - val_accuracy: 0.1800 - lr: 3.1250e-05\n",
      "Epoch 60/300\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 1.8021 - accuracy: 0.2000 - val_loss: 1.7823 - val_accuracy: 0.1800 - lr: 1.5625e-05\n",
      "✅ Meta-Classifier Trained & Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Simulated training data (Replace with actual predictions from base models)\n",
    "X_train_meta = np.random.rand(1000, 24)  # 1000 samples, 24 features (4 models × 6 softmax outputs)\n",
    "y_train_meta = np.random.randint(0, 6, size=(1000,))  # True categories\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "y_train_meta = tf.keras.utils.to_categorical(y_train_meta, num_classes=6)\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-5)\n",
    "\n",
    "meta_classifier.fit(X_train_meta, y_train_meta, \n",
    "                    epochs=300, batch_size=32, validation_split=0.2, \n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Save Model\n",
    "meta_classifier.save(\"meta_classifier.keras\")\n",
    "\n",
    "print(\"✅ Meta-Classifier Trained & Saved Successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
